{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22ovicXGLZPj"
      },
      "outputs": [],
      "source": [
        "!pip install -qq trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kna73-ZKoMV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset, Dataset as HFDataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import DPOTrainer,DPOConfig\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Configuration\n",
        "# =============================================================================\n",
        "MODEL_NAME = \"Qwen/Qwen2-1.5B-Instruct\"\n",
        "DATASET_NAME = \"gsm8k\"\n",
        "NUM_ITERATIONS = 16     # Total number of I-DPO cycles\n",
        "SAMPLES_PER_ITERATION = 1024 # Subset size per iteration (adjust as needed)\n",
        "EVAL_DATASET_SIZE = 128  #Size of eval dataset\n",
        "DPO_EPOCHS = 1             # Train for 1 epoch over the generated data\n",
        "BATCH_SIZE = 4             # DPO optimization batch size\n",
        "GENERATION_BATCH_SIZE = 32 # Batch size during the generation phase\n",
        "GRADIENT_ACCUMULATION_STEPS = 32\n",
        "LEARNING_RATE = 1e-5\n",
        "DPO_BETA = 0.2             # The temperature parameter for DPO\n",
        "MAX_LENGTH = 1024\n",
        "MAX_PROMPT_LENGTH = 512\n",
        "MAX_TARGET_LENGTH = 2048\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EujxQGkK5IZ"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(DATASET_NAME,'main',split='train')\n",
        "test_dataset = load_dataset(DATASET_NAME,'main',split='test')\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map = 'auto'\n",
        "    )\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "tokenizer.padding_side = 'left'\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model,lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPLOX-9BOwNq"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "client = OpenAI(api_key = OPENAI_API_KEY)\n",
        "\n",
        "class answer_verifier(BaseModel):\n",
        "    do_answers_match: bool\n",
        "\n",
        "class make_hints(BaseModel):\n",
        "    hints: List[str]\n",
        "    was_assistant_answer_correct:bool\n",
        "\n",
        "def match_answers(answer,gold_answer):\n",
        "\n",
        "    instructions = '''You will recieve the answer to a problem from a virtual assistant and the real answer. You will output True or False depending on whether the virtual assistant final answer is correct. (disregard the reasoning, all we care about is if the final answers match)'''\n",
        "    prompt = f'''Virtual assistant answer: {answer}\n",
        "    Real answer: {gold_answer}\n",
        "\n",
        "    Do the final answers match? Return True or False'''\n",
        "    response = client.responses.parse(\n",
        "        model = 'gpt-4.1',\n",
        "        instructions = instructions,\n",
        "        input = prompt,\n",
        "        text_format = answer_verifier\n",
        "    )\n",
        "    output  = response.output_parsed\n",
        "    return output.do_answers_match\n",
        "\n",
        "def get_hints_from_oracle(question,answer,gold_answer):\n",
        "\n",
        "    instructions = '''You will recieve a virtual assistant answer and a model answer. Your job is to provide a list of hints to strongly guide the model towards the correct answer, while preveting any errors it has made'''\n",
        "    prompt = f'''Quesiton:{question}\n",
        "    Virtual assistant answer: {answer}\n",
        "    Real answer: {gold_answer}\n",
        "\n",
        "    Please provide a list of hints to strongly guide the model towards the correct answer, while preventing any errors it has made'''\n",
        "\n",
        "    response = client.responses.parse(\n",
        "        model = 'gpt-4.1',\n",
        "        instructions = instructions,\n",
        "        input = prompt,\n",
        "        text_format = make_hints\n",
        "    )\n",
        "    output = response.output_parsed\n",
        "    return output.hints,output.was_assistant_answer_correct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4d_S_hKOwt1"
      },
      "outputs": [],
      "source": [
        "def format_prompt(question,hints = None):\n",
        "    if hints is not None:\n",
        "        question =  f\"Question: {question}\\nHints: {hints}\\nAnswer:\"\n",
        "    else:\n",
        "        question =  f\"Question: {question}\\nAnswer:\"\n",
        "\n",
        "    messages = [\n",
        "        {'role':'system','content':'Solve the problem step by step. Effectively utilize any hints given to you. Clearly state your answer'},\n",
        "        {'role':'user','content':question}\n",
        "    ]\n",
        "\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "\n",
        "def batch_generate(model,device,prompts,temp = 0.7):\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation = True,max_length=MAX_PROMPT_LENGTH).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=MAX_TARGET_LENGTH,\n",
        "            do_sample = True,\n",
        "            top_p = 0.95,\n",
        "            temperature = temp,\n",
        "        )\n",
        "\n",
        "    generated_tokens = outputs[:, inputs['input_ids'].shape[1]:]\n",
        "    generated_prompts = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "    return generated_prompts\n",
        "\n",
        "def create_dynamic_dpo_dataset(model,dataset_subset,verbose = False):\n",
        "    model.eval()\n",
        "    device = model.device\n",
        "\n",
        "    questions = dataset_subset['question']\n",
        "    answers = dataset_subset['answer']\n",
        "\n",
        "    prompts = [format_prompt(q) for q in questions]\n",
        "    print('Generating initial (rejected) answers)...')\n",
        "    rejected_answers = []\n",
        "    for i in tqdm(range(0,len(prompts),GENERATION_BATCH_SIZE)):\n",
        "        batch = prompts[i:i+GENERATION_BATCH_SIZE]\n",
        "        rejected_answers += batch_generate(model,device,batch)\n",
        "\n",
        "    print('Generating augmented prompts...')\n",
        "    augmented_prompts = []\n",
        "    rejected_correctness = []\n",
        "\n",
        "    total_rejected_correct = 0\n",
        "    for q,r,a in tqdm(zip(questions,rejected_answers,answers)):\n",
        "        hints,correct = get_hints_from_oracle(q,r,a)\n",
        "        rejected_correctness.append(correct)\n",
        "        if correct:\n",
        "            total_rejected_correct += 1\n",
        "        augmented_prompts.append(format_prompt(q,hints))\n",
        "\n",
        "    print('Generating augmented (chosen) answers...')\n",
        "    chosen_answers = []\n",
        "    for i in tqdm(range(0,len(augmented_prompts),GENERATION_BATCH_SIZE)):\n",
        "        batch = augmented_prompts[i:i+GENERATION_BATCH_SIZE]\n",
        "        chosen_answers += batch_generate(model,device,batch,temp = 0.2)\n",
        "\n",
        "    false_rejected_but_true_chosen_list = []\n",
        "    total_false_rejected_but_true_chosen = 0\n",
        "    total_false_rejected = 0\n",
        "    print('Verifying answers...')\n",
        "    for rc,ca,a in tqdm(zip(rejected_correctness,chosen_answers,answers)):\n",
        "        if not rc:\n",
        "            total_false_rejected+=1\n",
        "            is_correct = match_answers(ca,a)\n",
        "            if is_correct:\n",
        "                total_false_rejected_but_true_chosen+=1\n",
        "            false_rejected_but_true_chosen_list.append(is_correct)\n",
        "\n",
        "        else:\n",
        "            false_rejected_but_true_chosen_list.append(False)\n",
        "\n",
        "\n",
        "\n",
        "    preference_data = []\n",
        "    for p,c,r,frbtc in zip(prompts,chosen_answers,rejected_answers,false_rejected_but_true_chosen_list):\n",
        "        if frbtc:\n",
        "            preference_data.append({'prompt':p,'chosen':c,'rejected':r})\n",
        "    if len(preference_data) == 0:\n",
        "        print(\"NO PAIRS WERE CREATED\")\n",
        "    elif verbose:\n",
        "        print(f'CREATED {len(preference_data)} pairs')\n",
        "        sample = preference_data[0]\n",
        "        print('+++++++SAMPLE++++++++')\n",
        "        print(f\"Prompt: {sample['prompt']}\")\n",
        "        print('=====================')\n",
        "        print(f\"Chosen: {sample['chosen']}\")\n",
        "        print('=====================')\n",
        "        print(f\"Rejected: {sample['rejected']}\")\n",
        "        print('++++++++++++++++++++++')\n",
        "        print(f'Training_accuracy = {total_rejected_correct/len(questions)}')\n",
        "        print(f'Accepted_accuracy = {total_false_rejected_but_true_chosen/total_false_rejected}')\n",
        "\n",
        "    return preference_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QgnFAHFtsK0"
      },
      "outputs": [],
      "source": [
        "eval_dataset = test_dataset.shuffle(seed = 42).select(range(EVAL_DATASET_SIZE))\n",
        "eval_questions = [format_prompt(q) for q in eval_dataset['question']]\n",
        "eval_answers = [a for a in eval_dataset['answer']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77nhC5RLWAMk"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model,verbose = False):\n",
        "    model.eval()\n",
        "    device = model.device\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    print(f'Evaluating model on {len(eval_questions)} questions')\n",
        "\n",
        "    model_answers = []\n",
        "    for i in tqdm(range(0,len(eval_questions),GENERATION_BATCH_SIZE)):\n",
        "        batch = eval_questions[i:i+GENERATION_BATCH_SIZE]\n",
        "        model_answers += batch_generate(model,device,batch, temp = 0.01)\n",
        "\n",
        "    counter = 0\n",
        "    for q,ma,a in zip(eval_questions,model_answers,eval_answers):\n",
        "        is_correct = match_answers(ma,a)\n",
        "        if verbose and counter < 2:\n",
        "            counter += 1\n",
        "            print(f'++++++++QUESTION {total+1}+++++++++/n')\n",
        "            print(f'Question: {q}')\n",
        "            print('=========')\n",
        "            print(f'Generated answer: {ma}')\n",
        "            print('=========')\n",
        "            print(f'Correct answer: {a}')\n",
        "            print('=========')\n",
        "            print(f'Is Correct?: {is_correct}')\n",
        "            print('=========')\n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "    return correct/total\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaNx-JdhxbMZ"
      },
      "outputs": [],
      "source": [
        "print('\\nStarting iterative DPO training...')\n",
        "\n",
        "eval_accuracy = evaluate_model(model,verbose = True)\n",
        "print('BASELINE ACCURACY')\n",
        "print(f'Evaluation accuracy: {eval_accuracy}')\n",
        "for iteration in range(NUM_ITERATIONS):\n",
        "    print(f'\\nStarting iteration {iteration+1}/{NUM_ITERATIONS}')\n",
        "\n",
        "    dataset_subset = dataset.shuffle(seed = 42 + iteration).select(range(min(SAMPLES_PER_ITERATION,len(dataset))))\n",
        "\n",
        "    tokenizer.padding_side = 'left'\n",
        "    preference_dataset_list = create_dynamic_dpo_dataset(model,dataset_subset,verbose = True)\n",
        "\n",
        "    preference_df = pd.DataFrame(preference_dataset_list)\n",
        "    train_dataset = HFDataset.from_pandas(preference_df)\n",
        "\n",
        "    print(f\"Generated {len(preference_df)} preference pairs\")\n",
        "\n",
        "    tokenizer.padding_side = 'right'\n",
        "    training_args = DPOConfig(\n",
        "        output_dir = f'./idpo_output_iteration{iteration+1}',\n",
        "        learning_rate = LEARNING_RATE,\n",
        "        per_device_train_batch_size = BATCH_SIZE,\n",
        "        gradient_accumulation_steps = GRADIENT_ACCUMULATION_STEPS,\n",
        "        num_train_epochs = DPO_EPOCHS,\n",
        "        logging_steps = 10,\n",
        "        remove_unused_columns = False, #important for dpo\n",
        "        optim = 'adamw_torch',\n",
        "        save_strategy = 'no',\n",
        "        lr_scheduler_type = 'cosine', #lr_scheduler_type instead of lr_scheduler\n",
        "        warmup_ratio = 0.1,\n",
        "        beta = DPO_BETA,\n",
        "    )\n",
        "\n",
        "    dpo_trainer = DPOTrainer(\n",
        "        model = model,\n",
        "        ref_model = None,\n",
        "        args = training_args,\n",
        "        # peft_config = lora_config,\n",
        "        train_dataset = train_dataset,\n",
        "        processing_class = tokenizer,\n",
        "        # max_length = MAX_LENGTH,\n",
        "        # max_prompt_length = MAX_PROMPT_LENGTH,\n",
        "        # max_target_length = MAX_TARGET_LENGTH,\n",
        "    )\n",
        "\n",
        "    dpo_trainer.train()\n",
        "\n",
        "    model = dpo_trainer.model\n",
        "\n",
        "    tokenizer.padding_side = 'left'\n",
        "    eval_accuracy = evaluate_model(model,verbose = True)\n",
        "    print(f'Evaluation accuracy: {eval_accuracy}')\n",
        "\n",
        "    del dpo_trainer, train_dataset, preference_df, preference_dataset_list\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}