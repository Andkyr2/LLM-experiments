# Iterative DPO with AI Feedback for Mathematical Reasoning on GSM8K

This repository contains code for an experiment exploring iterative Direct Preference Optimization (DPO) on a small language model for improving mathematical reasoning capabilities. The method leverages a powerful external oracle model (GPT-4) to generate corrective feedback on a sample-by-sample basis, creating high-quality preference pairs dynamically.

## 1\. Abstract

Large Language Models (LLMs) often struggle with multi-step reasoning tasks, especially smaller models. This experiment investigates whether a smaller model's reasoning abilities can be enhanced through an iterative fine-tuning process. We employ Direct Preference Optimization (DPO) on the **Qwen/Qwen2-1.5B-Instruct** model, focusing on the **GSM8K** benchmark dataset. The core methodology involves generating preference data dynamically: for each training sample where the base model fails, we use a superior oracle model (**GPT-4.1**) to generate corrective hints. The base model then re-attempts the problem with these hints. A preference pair (chosen, rejected) is created only if the initial attempt failed and the hint-guided attempt succeeded. This ensures the model learns specifically from its mistakes. The model undergoes 16 planned iterations of this "generate-feedback-train" cycle. Initial results show a fluctuating but generally positive trend in accuracy on a holdout test set, demonstrating the potential of oracle-guided iterative DPO for complex reasoning tasks, though highlighting challenges in training stability.

## 2\. Methodology

The experiment follows an iterative self-improvement loop, where the model is continuously fine-tuned on data generated by evaluating its own performance against gold solutions.

### 2.1. Core Components

  * **Base Model:** `Qwen/Qwen2-1.5B-Instruct`. Training is performed efficiently using Parameter-Efficient Fine-Tuning (PEFT) with LoRA adapters (`r=16`, `lora_alpha=32`).
  * **Training Dataset:** `gsm8k` (train split), a dataset of grade school math word problems.
  * **Oracle Model:** `gpt-4.1` (via OpenAI API) used for answer verification and feedback generation.

### 2.2. Iterative Training Loop

The training process consists of 16 iterations. Each iteration performs the following steps on a newly sampled subset of 1024 problems from the GSM8K dataset:

**Step 1: Initial Generation (Rejected Candidate)**
For a given question $q$, the current iteration of the Qwen2 model generates an initial answer $a\_{initial}$. This answer will serve as the potential **rejected** response in the preference pair.

```python
# Pseudo-code for initial generation
prompt = format_prompt(question)
rejected_answer = model.generate(prompt, temperature=0.7)
```

**Step 2: Oracle Feedback Generation**
The generated $a\_{initial}$ is compared to the ground truth solution $a\_{gold}$.

  * If $a\_{initial}$ is correct, the sample is discarded for this iteration.
  * If $a\_{initial}$ is incorrect, it is passed to the GPT-4 oracle. The oracle's task is to analyze the error in $a\_{initial}$ and generate a list of targeted **hints** to guide the model toward the correct reasoning path.

<!-- end list -->

```python
# Pseudo-code for oracle call and hint generation
if not match_answers(rejected_answer, gold_answer):
    hints, _ = get_hints_from_oracle(question, rejected_answer, gold_answer)
```

**Step 3: Corrective Generation (Chosen Candidate)**
A new, augmented prompt $p\_{aug}$ is created by appending the generated hints to the original question $q$. The Qwen2 model generates a new answer $a\_{corrected}$ using this augmented prompt, typically with a lower temperature to encourage more deterministic output. This $a\_{corrected}$ serves as the potential **chosen** response.

```python
# Pseudo-code for corrective generation
augmented_prompt = format_prompt(question, hints=hints)
chosen_answer = model.generate(augmented_prompt, temperature=0.2)
```

**Step 4: Preference Pair Filtering**
To ensure high-quality training signal, we apply a strict filter: a preference pair $(p, a\_{chosen}, a\_{rejected})$ is added to the training set **only if** the hint-guided answer $a\_{corrected}$ successfully matches the gold solution $a\_{gold}$. This filtering mechanism ensures the model only learns from successful corrections, preventing reinforcement of poor reasoning that might occur even with hints.

**Step 5: DPO Fine-tuning**
The collected set of filtered preference pairs is used to fine-tune the Qwen2 model for one epoch using DPO. The model from this iteration becomes the base model for the next iteration.

### 2.3. Hyperparameters

  * **Model:** `Qwen/Qwen2-1.5B-Instruct`
  * **LoRA Rank (`r`):** 16
  * **LoRA Alpha (`lora_alpha`):** 32
  * **DPO Beta ($\\beta$):** 0.2
  * **Learning Rate:** 1e-5
  * **Batch Size:** 4 per device
  * **Gradient Accumulation Steps:** 32
  * **Iterations:** 16 planned
  * **Samples per Iteration:** 1024

## 3\. Results and Discussion

The experiment was executed, and results were tracked over several iterations before being halted by an external factor.

### 3.1. Evaluation Performance

Model accuracy was evaluated on a holdout set of 128 questions from the `gsm8k` test split after each training iteration.

| Iteration | Evaluation Accuracy | Change from Previous | Number of Generated Pairs | Accepted Accuracy\* |
| :--- | :--- | :--- | :--- | :--- |
| **Baseline** | 54.69% | - | - | - |
| Iteration 1 | 55.47% | +0.78% | 240 | 73.39% |
| Iteration 2 | 53.91% | -1.56% | 289 | 75.06% |
| Iteration 3 | 53.13% | -0.78% | 298 | 74.50% |
| Iteration 4 | 60.94% | +7.81% | 294 | 75.77% |
| Iteration 5 | 57.03% | -3.91% | 326 | 73.26% |
| Iteration 6 | Halted | - | 330 | 75.69% |

*\*Accepted Accuracy: The percentage of incorrect initial generations that were successfully corrected using hints, thereby forming a valid training pair.*

### 3.2. Discussion of Results

1.  **Performance Fluctuation:** The evaluation accuracy did not increase monotonically. After an initial slight improvement, performance dipped before showing a significant spike in **Iteration 4** to nearly 61%. This suggests that the model successfully integrated new reasoning patterns but also highlights potential training instability. The drop in Iteration 5 further supports this; the model might be "forgetting" previously learned capabilities or overfitting to the specific patterns in the most recent data batch.

2.  **Effectiveness of Feedback Mechanism:** The "Accepted Accuracy" metric remained consistently high (around 73-76%). This indicates that for approximately three-quarters of the problems where the model initially failed, providing oracle-generated hints was sufficient to guide the model to the correct solution. This confirms that the dynamic data generation process created a strong and relevant learning signal.

3.  **Experimental Limitations:** The experiment was terminated prematurely during the evaluation phase of Iteration 6 due to an `openai.RateLimitError: insufficient_quota`. This prevented the completion of all 16 planned iterations, making it difficult to determine if the performance fluctuations would have stabilized into a long-term upward trend.

## 4\. Conclusion

This experiment successfully implemented a dynamic iterative DPO framework for enhancing mathematical reasoning in a small LLM. The core hypothesis—that an oracle model can provide effective corrective feedback for targeted improvement—is supported by the high "Accepted Accuracy" rate, demonstrating successful generation of useful training pairs.

While a significant performance peak was observed, the volatility across iterations suggests that further research is needed to stabilize training. Potential future work includes:

  * Running the experiment for all planned iterations.
  * Tuning DPO hyperparameters (e.g., learning rate and $\\beta$) to mitigate instability.
  * Increasing the size of the dataset subset per iteration to promote more stable learning gradients.
  * Exploring more sophisticated feedback mechanisms beyond simple hints.
